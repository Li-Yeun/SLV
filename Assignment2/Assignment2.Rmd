---
title: "Supervised Learning and Visualisation"
subtitle: "Assignment 2: Prediction models"
author: "Abdool Al-Khaledi, Amalia Tsakali and Willem van Veluw"
date: "`r Sys.Date()`"
output: html_document
---
# Introduction
### The dataset
```{r, include = FALSE}
library(tidyverse)
library(cowplot)
library(mice)
library(rpart)
library(rpart.plot)
library(randomForest)
library(pROC)
library(mlbench)
data("PimaIndiansDiabetes2")

set.seed(45)
```

### Missing values
When exploring the dataset, we observed that there are quite some missing values in the dataset. The number of missing values per variable is computed below. We see that the variable `insulin` contains a lot of missing values: almost 50% of the observations has no record for insulin. We can therefore not simply throw away all rows with a missing value, it is a too large fraction of the dataset. We have to deal with the missing values in a different way.

```{r, echo = FALSE}
nr_nas <- 1:9
names(nr_nas) <- colnames(PimaIndiansDiabetes2)
for(i in 1:ncol(PimaIndiansDiabetes2)){
  nr_nas[i] <- sum(is.na(PimaIndiansDiabetes2[,i]))
}
"Number of missing values in the dataset:"
nr_nas[-9]
```
Let us first inspect the missing data in a little bit more detail. From the missing value pattern plot we see below, we can draw equivalent conclusions as from the table above. The variable `insulin` is the most problematic one, followed by `triceps` and `pressure`. There are 652 missing values in total. 

```{r, echo = FALSE}
mice::md.pattern(PimaIndiansDiabetes2)
```
We decided to work around the problem of missing values by imputing them: the package `mice` has a nice function for this. Credits of this approach go to Gerko Vink who pointed us in the right direction and helped us a lot by providing some example code. After imputing the missing values, there are none left as can be seen from the table below. We can now start fitting our models and make predictions!

```{r}
data_complete <- PimaIndiansDiabetes2 %>% 
  mice(m = 1, maxit = 100, printFlag = FALSE) %>% 
  complete()
```

```{r, echo = FALSE}
nr_nas_complete <- 1:9
names(nr_nas_complete) <- colnames(data_complete)
for(i in 1:ncol(data_complete)){
  nr_nas_complete[i] <- sum(is.na(data_complete[,i]))
}
"Missing values in the dataset after impution:"
nr_nas_complete[-9]
```
###Summary Statistics before and after imputation
To determine whether the imputation has altered the properties of our dataset we will extract some summary statistics, and create some exploratory plots concerning the distribution of each value in healthy and diseased individuals. 
```{r}
summary(PimaIndiansDiabetes2)

summary(data_complete)

data_complete %>%
  pivot_longer(where(is.numeric)) %>%
  ggplot(aes(x = value, col = diabetes, fill = diabetes)) +
  geom_density(alpha = 0.8) +
  facet_wrap(~name, scales = "free") +
  scale_color_brewer(palette = "Paired") +
  scale_fill_brewer(palette = "Paired") +
  theme_minimal()

PimaIndiansDiabetes2 %>%
  pivot_longer(where(is.numeric)) %>%
  ggplot(aes(x = value, col = diabetes, fill = diabetes)) +
  geom_density(alpha = 0.8) +
  facet_wrap(~name, scales = "free") +
  scale_color_brewer(palette = "Paired") +
  scale_fill_brewer(palette = "Paired") +
  theme_minimal()


```
Looking at the density plots, as well as the summary statistics, we can conclude that imputing the missing values has not altered the distribution of our data. So we believe it is safe to use the imputed dataset from this point on.

### Creating training and testing sets
Before we start training our models we divide the dataset into a training and a test set. For the training set we will use 80% of the data (614 observations) and the test set will be formed by the remaining 20% (154 observations). For training the trees (see next section) we will perform a 10-fold cross-validation. Hence, the 614 training observations will be divided in six folds of 61 observations and four folds of 62 observations.

```{r}
data_complete <- data_complete %>% 
  mutate(Set = sample( c(rep("Train", 614), rep("Test", 154))))

data_train <- data_complete %>% 
  filter(Set == "Train") %>% 
  mutate(Fold = sample( c(rep(1:6, each = 61), rep(7:10, each = 62))))
data_test <- data_complete %>% filter(Set == "Test")
```

### Models and Metrics
<p style="color:red;">
Write something about the models we will consider: simple (logistic regression and classification tree) and complex (random forest). Stress that we will use a cross-validation to train the tree and the random forest. </p>

<p style="color:red;">
Write something about the way we will compare the performance of our models. State the metrics we will compute from the confusion matrix, say that we also look at AUC and we will ultimately use a McNemar's test. Maybe give a short explanation of the McNemar test, how it works.
</p>

# Simple model: Logistic regression
### Approach
Logistic Regression (LR) is a classification method that models the probability of an observation by the logit-link function. The probability of observation $\vec{x}$ to belong at class 1 ($y=1$) is modelled as
$$
  P(y = 1|x) = \frac{1}{1+exp(-\vec{\beta}\bullet\vec{x})},
$$
where $\vec{\beta}\bullet\vec{x}=\beta_0+\beta_1\cdot x_1+...+\beta_J\cdot x_J$. Then it follows that class 0 has a probability of
$$
\begin{align*}
  P(y = 0|x) & = 1-P(y=1|x)\\ 
    & = \frac{1}{1+exp(\vec{\beta}\bullet\vec{x})}.
\end{align*}
$$
When a LR model is fit, the coefficients $\vec{\beta}=\beta_0,...,\beta_J$ are estimated. With these estimates in place, the class of a (new) observation is predicted as the one with largest probability.  
For a simple prediction rule, the log-odds form a nice tool. The log-odds are defined as
$$
\begin{align*}
  \log(\frac{P(y=1|x)}{P(y=0|x)}) & = \log(exp(\vec{\beta}\bullet\vec{x}))\\
  & = \vec{\beta}\bullet\vec{x}.
\end{align*}
$$
Now, when the probability of class 1 is largest, the log-odds are positive. In the other case, where the probability of class 0 is largest, the log-odds are negative. This induces a simple prediction rule for LR: predict class 1 if $\vec{\beta}\bullet\vec{x}\geq0$, and class 0 otherwise.

### Training Results
We have trained a LR model with all five variables as predictors. The estimated coefficients can be seen below. From the estimated coefficients we see that all predictors have a positive influence of the class, i.e. if either variable increases, the probability of the observation being of class 1 increases as well.
```{r, include = FALSE}
lr_mod <- glm(formula = diabetes ~.,
              data = data_train %>% select(-Set, -Fold),
              family = binomial)
```
```{r, echo = FALSE}
lr_mod
```
To visualise the influence of a certain variable on the prediction probability, we took the following procedure.

(1) To investigate the influence of one variable, we fix all other variables at their mean.
(2) Create a sequence of 1000 points between the minimum and maximum observed value of the variable of interest.
(3) Compute the prediction probability for every point: we obtain 1000 prediction probabilities.

<p style="color:red;">
Say something about the plots. Note that all variables are rather large, whereas `pedigree` is a decimal value.
</p>


These plots are in accordance with the coefficient values we extracted from the model. Namely, we see see a decrease in probability when the values of predictors with negative coefficients increase, and an increase for the predictors that have a positive coefficient.
We also see that predictors whose coefficient is high in absolute value, like pedigree, display a detectable change in probability inside a small range of values. And vice versa, predictors with a low absolute value coefficient, like insulin, need a larger increase in their value for a visible change in probability. This becomes apparent by looking at the x axes. (Note that x axis ranges from 0 to 2.5). 
One surprising thing about these plots is that we see a linear relationship between the probability and many predictors, when we would expect a sigmoid curve, similar to the one for glucose.

```{r, include = FALSE}
means <- data_train %>% 
  select(-diabetes,-Set) %>% 
  apply(MARGIN = 2, FUN = mean)

plot_data <- list()
for(i in 1:8){
  x <- seq(min(data_train[,i]), max(data_train[,i]), length.out = 1000)
  data <- matrix( means[-i] %>% rep(each = 1000), nrow = 1000)
  data <- data.frame(x,data)
  colnames(data) <- c(names(means[i]), names(means[-i]))
  
  y <- predict(lr_mod, newdata = data, type = "response")
  
  plot_data[[i]] <- data.frame(x,y)
}

plot_pregnant <- plot_data[[1]] %>% 
  ggplot(aes(x = x, y = y)) +
  geom_line() +
  labs(x = "pregnant",
       y = "probability") +
  ylim(0,1) +
  theme_minimal()

plot_glucose <- plot_data[[2]] %>% 
  ggplot(aes(x = x, y = y)) +
  geom_line() +
  labs(x = "glucose",
       y = "probability") +
  ylim(0,1) +
  theme_minimal()

plot_pressure <- plot_data[[3]] %>% 
  ggplot(aes(x = x, y = y)) +
  geom_line() +
  labs(x = "pressure",
       y = "probability") +
  ylim(0,1) +
  theme_minimal()

plot_triceps <- plot_data[[4]] %>% 
  ggplot(aes(x = x, y = y)) +
  geom_line() +
  labs(x = "triceps",
       y = "probability") +
  ylim(0,1) +
  theme_minimal()

plot_insulin <- plot_data[[5]] %>% 
  ggplot(aes(x = x, y = y)) +
  geom_line() +
  labs(x = "insulin",
       y = "probability") +
  ylim(0,1) +
  theme_minimal()

plot_mass <- plot_data[[6]] %>% 
  ggplot(aes(x = x, y = y)) +
  geom_line() +
  labs(x = "mass",
       y = "probability") +
  ylim(0,1) +
  theme_minimal()

plot_pedigree <- plot_data[[7]] %>% 
  ggplot(aes(x = x, y = y)) +
  geom_line() +
  labs(x = "pedigree",
       y = "probability") +
  ylim(0,1) +
  theme_minimal()

plot_age <- plot_data[[8]] %>% 
  ggplot(aes(x = x, y = y)) +
  geom_line() +
  labs(x = "age",
       y = "probability") +
  ylim(0,1) +
  theme_minimal()
```

```{r, echo = FALSE}
plot_grid(plot_pregnant, plot_glucose, plot_pressure,
          plot_triceps, plot_insulin, plot_mass,
          plot_pedigree, plot_age)
```

### Performance Results
To be able to compare the performance of LR with other models, we construct the confusion matrix of predictions on the test data. From this confusion matrix, we compute the metrics that we think are most important.

```{r, echo = FALSE}
lr_pred_probs <- predict(lr_mod, newdata = data_test, type = "response")
lr_preds <- factor(lr_pred_probs > .5, labels = c("neg", "pos"))

lr_confmat <- table(True = data_test$diabetes, Predicted = lr_preds)
lr_confmat
#calculate the most important metrics
TN <- lr_confmat[1, 1]
FN <- lr_confmat[2, 1]
FP <- lr_confmat[1, 2]
TP <- lr_confmat[2, 2]

tibble(
  Acc = (TP + TN) / sum(lr_confmat),
  TPR = TP / (TP + FN),
  TNR = TN / (TN + FP),
  FPR = FP / (TN + FP),
  PPV = TP / (TP + FP),
  NPV = TN / (TN + FN)
)

#ROC
roc_lr <- roc(data_test$diabetes, lr_pred_probs)
ggroc(roc_lr) + theme_minimal() + labs(title = "Logistic Regression")

roc_lr
```


# Simple model: Classification Tree
### Approach
The second model that we will consider is the classification tree. The idea of such a tree is to recursively make splits based on the variables. Each split should make the observations within one of the resulting groups as similar as possible. The prediction rule of the tree is defined as "majority vote": within a leaf node, determine the class with maximal frequency among the observation within that node. If two or more classes have equal frequency, then choose uniformly random.  
An important notion to make when considering classification trees is the tree complexity. A classification tree has a high complexity when it makes a lot of splits, resulting in a large/deep tree with many leaf nodes that contain few observations. In that case, there is a high variance among the predictions. To prevent this, one can *prune* the tree: stop splitting when some condition is met. For instance, one could

- stop splitting when the number of observations in a node is below some threshold $n_{min}$.
- stop splitting when the complexity of the tree is larger then some threshold $cp$.

Note that there are more ways to prune a tree (or complexity parameters to specify). A list of all complexity parameters and their default values can be found in the documentation of `rpart.control`. Note the default values $n_{min} = 20$ and $cp = 0.01$.  
We will tune the *hyperparameters* $n_{min}$ and $cp$ by means of the 10-fold cross-validation as specified earlier. The metric of performance that we will use is the accuracy. For the possible values of the hyperparameters, we consider $56$ combinations:

- $n_{min}\in\{50\cdot i|i=0,...,6\}$, as $n_{min}=0$ resembles the full grown tree and $n_{min}=300$ gives a tree with just one split (with all other hyperparameters equal to their default value).
- $cp\in\{0.01\cdot i|i=0,...,7\}$, as $cp=0$ resembles the full grown tree and $cp=0.07$ gives a tree with just one split (with all other hyperparameters equal to their default value).

### Training Results
The results of the 10-fold cross-validation can be found below. For the implementation of the cross-validation, please read the .Rmd file. From the table we can read that the tree with $n_{min}=50$ and $cp = 0$ performs best: its mean accuracy over the folds equals approximately 0.76. With the hyperparameters set, we fit the classification tree again on the entire dataset. The resulting tree can be found below the table.


```{r, include = FALSE}
nmins <- seq(0,300, by = 50)
cps <- seq(0,0.07, by = 0.01)

accuracies <- matrix(nrow = length(nmins), ncol = length(cps))
colnames(accuracies) <- paste("cp =", as.character(cps))
rownames(accuracies) <- paste("n_min =", as.character(nmins))

for(i in 1:length(nmins)){
  for(j in 1:length(cps)){
    mean_acc <- 0
    for(k in 1:10){
      train <- data_train %>% filter(Fold != k) %>% select(-Set, -Fold)
      val <- data_train %>% filter(Fold == k)
      ctree <- rpart(diabetes ~.,
                     data = train,
                     control = rpart.control(minsplit = nmins[i], cp = cps[j]))
      preds <- predict(ctree, newdata = val, type = "class")
      acc <- sum(val$diabetes == preds)/nrow(val)
      mean_acc <- mean_acc + acc/10
    }
    accuracies[i,j] <- mean_acc
  }
}
```

```{r, echo = FALSE}
"Mean accuracy over 10 folds. The rows represent n_min, the columns represent cp."
accuracies

hyperparams <- which(accuracies == max(accuracies), arr.ind = TRUE)
ctree_mod <- rpart(diabetes ~.,
                   data = data_train %>% select(-Set, -Fold),
                   control = rpart.control(minsplit = (hyperparams[1]-1)*50, cp = (hyperparams[2]-1)*0.01))
rpart.plot(ctree_mod)
```

It is interesting to see that the classification tree does multiple splits on `glucose`. It seems that this variable contains a lot of helpful information. We see that idea confirmed by the computed variable importance, see the barplot below.  
The second thing that we noted is the following. Although `insulin` is the second most important variable, the tree does not perform a split on that variable. We are unsure why this happens.  
The last interesting notion is that the tree predicts an observation with `pregnant` greater than 7 and `glucose` between 96 and 124 as *diabetes*. We see that the leaf in which this observation ends up contains 8% of the training observations (about 49 observations). Among this 49 observations, there is a slight majority with class *diabetes*.

```{r, echo = FALSE}
data.frame(Variable = names(ctree_mod$variable.importance),
                           Importance = ctree_mod$variable.importance) %>% 
  ggplot(aes(x = Variable, y = Importance, fill = Variable)) +
  geom_bar(stat = "identity", show.legend = FALSE) +
  labs(title = "Variable importance for the classification tree.") +
  theme_minimal()
```

### Performance results
To be able to compare the performance of LR with other models, we construct the confusion matrix of predictions on the test data. From this confusion matrix, we compute the metrics that we think are most important.

```{r, echo = FALSE}
ctree_preds <- predict(ctree_mod, newdata = data_test, type = "class")

ctree_confmat <- table(True = data_test$diabetes, Predicted = ctree_preds)
ctree_confmat
#calculate the most important metrics
TN_tree <- ctree_confmat[1, 1]
FN_tree <- ctree_confmat[2, 1]
FP_tree <- ctree_confmat[1, 2]
TP_tree <- ctree_confmat[2, 2]

tibble(
  Acc_tree = (TP_tree + TN_tree) / sum(ctree_confmat),
  TPR_tree = TP_tree / (TP_tree + FN_tree),
  TNR_tree = TN_tree / (TN_tree + FP_tree),
  FPR_tree = FP_tree / (TN_tree + FP_tree),
  PPV_tree = TP_tree / (TP_tree + FP_tree),
  NPV_tree = TN_tree / (TN_tree + FN_tree)
)
#ROC
ctree_preds_prob <- predict(ctree_mod, newdata = data_test)
class(ctree_preds_prob)
roc_ctree <- roc(data_test$diabetes, ctree_preds_prob[,2])
ggroc(roc_ctree) + theme_minimal() + labs(title = "Classification tree")

roc_ctree
```

###Comparison between models
```{r}

```

