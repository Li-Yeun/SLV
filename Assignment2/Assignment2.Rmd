---
title: "Supervised Learning and Visualisation"
subtitle: "Assignment 2: Prediction models"
author: "Abdool Al-Khaledi, Amalia Tsakali and Willem van Veluw"
date: "`r Sys.Date()`"
output: html_document
---
# Introduction
The ability to cater preventive, diagnostic and treatment measures on a case-by-case bases despite the inherent variability present in individuals has widely been considered as the holy grail of modern medicine. However, decoding this inherent variability across a wide range of possible attributes that might effect a persons health was a non-trivial issue, which has since been explored by leveraging machine learning algorithms. To that end, the purpose of this study is to investigate the performance of several supervised learning algorithms at a diagnostic (diabetic vs. non-diabetic) classification task. Specifically, we will asses the performance of a Logistic Regression model (linear classifier), a classification tree (non-linear classier) and a Random Forest (ensemble of non-linear classifiers) at this task. The accuracy values of the models will be compared via the McNemar test in order to assess the performance of the linear vs non-linear models. As well as to determine if the performance of the ensemble model was significantly different from the individual models. 


### The dataset
The data-set under analysis for the purpose of this study is the "Pima Indians Diabetes Database", downloaded from Kaggle. The data was originally curated by the U.S based National Institute of Diabetes and Digestive and Kidney Diseases. The data-set contains 768 observations with each observation representing an individual. Along with 9 columns, where 8 of these columns are predictor variables and the final "diabetes" column states the class label (positive or negative). The label here refers to whether or not a person is diabetic. It is important to note that the data set acquired from Kaggle is a subset of the NIH database. In this study, all the observations refer to females at least 21 years of age and of Pima Indian heritage.The predictor variables include: The number of times pregnant ('preg'),the diastolic blood pressure ('pressure'), tricep skin-fold thickness ('triceps'), serum-insulin ('insulin'), B.M.I ('mass'), a measure of diabetes presence in the pedigree chart ('pedigree'), and age in years ('age'). 


```{r, include = FALSE}
library(tidyverse)
library(caret)
library(cowplot)
library(mice)
library(rpart)
library(rpart.plot)
library(randomForest)
library(pROC)
library(mlbench)
data("PimaIndiansDiabetes2")

set.seed(45)
```

### Missing values
When exploring the dataset, we observed that there are quite some missing values in the dataset. The number of missing values per variable is computed below. We see that the variable `insulin` contains a lot of missing values: almost 50% of the observations has no record for insulin. We can therefore not simply throw away all rows with a missing value, it is a too large fraction of the dataset. We have to deal with the missing values in a different way.

```{r, echo = FALSE}
nr_nas <- 1:9
names(nr_nas) <- colnames(PimaIndiansDiabetes2)
for(i in 1:ncol(PimaIndiansDiabetes2)){
  nr_nas[i] <- sum(is.na(PimaIndiansDiabetes2[,i]))
}
"Number of missing values in the dataset:"
nr_nas[-9]
```
Let us first inspect the missing data in a little bit more detail. From the missing value pattern plot we see below, we can draw equivalent conclusions as from the table above. The variable `insulin` is the most problematic one, followed by `triceps` and `pressure`. There are 652 missing values in total. 

```{r, echo = FALSE}
mice::md.pattern(PimaIndiansDiabetes2)
```
We decided to work around the problem of missing values by imputing them: the package `mice` has a nice function for this. Credits of this approach go to Gerko Vink who pointed us in the right direction and helped us a lot by providing some example code. After imputing the missing values, there are none left as can be seen from the table below. We can now start fitting our models and make predictions!

```{r}
data_complete <- PimaIndiansDiabetes2 %>% 
  mice(m = 1, maxit = 100, printFlag = FALSE) %>% 
  complete()
```

```{r, echo = FALSE}
nr_nas_complete <- 1:9
names(nr_nas_complete) <- colnames(data_complete)
for(i in 1:ncol(data_complete)){
  nr_nas_complete[i] <- sum(is.na(data_complete[,i]))
}
"Missing values in the dataset after impution:"
nr_nas_complete[-9]
```

### Summary statistics
To determine whether the imputation has altered the properties of our dataset we will extract some summary statistics, and create some exploratory plots concerning the distribution of each value in healthy and diseased individuals. 
```{r}
"The dataset before imputation"
summary(PimaIndiansDiabetes2)

"The dataset after imputation"
summary(data_complete)

PimaIndiansDiabetes2 %>%
  pivot_longer(where(is.numeric)) %>%
  ggplot(aes(x = value, col = diabetes, fill = diabetes)) +
  geom_density(alpha = 0.8) +
  facet_wrap(~name, scales = "free") +
  scale_color_brewer(palette = "Paired") +
  scale_fill_brewer(palette = "Paired") +
  labs(title = "The dataset before imputation.",
       subtitle = "Note that 652 rows have been ommited because of the missing values.") +
  theme_minimal()

data_complete %>%
  pivot_longer(where(is.numeric)) %>%
  ggplot(aes(x = value, col = diabetes, fill = diabetes)) +
  geom_density(alpha = 0.8) +
  facet_wrap(~name, scales = "free") +
  scale_color_brewer(palette = "Paired") +
  scale_fill_brewer(palette = "Paired") +
  labs(title = "The dataset after imputation.",
       subtitle = "No rows are omitted, since there are no missing values anymore.") +
  theme_minimal()


```
Looking at the density plots, as well as the summary statistics, we can conclude that imputing the missing values has not altered the distribution of our data. So we believe it is safe to use the imputed dataset from this point on.

### Creating training and testing sets
Before we start training our models we divide the dataset into a training and a test set. For the training set we will use 80% of the data (614 observations) and the test set will be formed by the remaining 20% (154 observations). For training the trees (see next section) we will perform a 10-fold cross-validation. Hence, the 614 training observations will be divided in six folds of 61 observations and four folds of 62 observations.

```{r}
data_complete <- data_complete %>% 
  mutate(Set = sample( c(rep("Train", 614), rep("Test", 154))))

data_train <- data_complete %>% 
  filter(Set == "Train") %>% 
  mutate(Fold = sample( c(rep(1:6, each = 61), rep(7:10, each = 62))))
data_test <- data_complete %>% filter(Set == "Test")
```

### Models and Metrics
The McNemar test was employed in order to compare the performance of our classifiers via their accuracy score. This non-parametric statistical test receives input in the form of a 2x2 contingency table and is produced by tallying the number of correct and incorrect predictions made by the two models under investigation. Worth noting, The null hypothesis states that the row and column marginal frequencies of the contingency table are not statistically different. A positive McNemar test which denotes a significant difference is reached when the p-value is less than 0.05, at a 95 % confidence interval. First,we will compare the performance of the non-linear classifiers: Classification Tree and Random Forest against the logistic regression model. Furthermore, we will compare the performance of the single classification tree against the more complex Random forest algorithm. 

# Simple model: Logistic regression
### Approach
Logistic Regression (LR) is a classification method that models the probability of an observation by the logit-link function. The probability of observation $\vec{x}$ to belong at class 1 ($y=1$) is modelled as
$$
  P(y = 1|x) = \frac{1}{1+exp(-\vec{\beta}\bullet\vec{x})},
$$
where $\vec{\beta}\bullet\vec{x}=\beta_0+\beta_1\cdot x_1+...+\beta_J\cdot x_J$. Then it follows that class 0 has a probability of
$$
\begin{align*}
  P(y = 0|x) & = 1-P(y=1|x)\\ 
    & = \frac{1}{1+exp(\vec{\beta}\bullet\vec{x})}.
\end{align*}
$$
When a LR model is fit, the coefficients $\vec{\beta}=\beta_0,...,\beta_J$ are estimated. With these estimates in place, the class of a (new) observation is predicted as the one with largest probability.  
For a simple prediction rule, the log-odds form a nice tool. The log-odds are defined as
$$
\begin{align*}
  \log(\frac{P(y=1|x)}{P(y=0|x)}) & = \log(exp(\vec{\beta}\bullet\vec{x}))\\
  & = \vec{\beta}\bullet\vec{x}.
\end{align*}
$$
Now, when the probability of class 1 is largest, the log-odds are positive. In the other case, where the probability of class 0 is largest, the log-odds are negative. This induces a simple prediction rule for LR: predict class 1 if $\vec{\beta}\bullet\vec{x}\geq0$, and class 0 otherwise.

### Training Results
We have trained a LR model with all five variables as predictors. The estimated coefficients can be seen below. From the estimated coefficients we see that all predictors have a positive influence of the class, i.e. if either variable increases, the probability of the observation being of class 1 increases as well.
```{r, include = FALSE}
lr_mod <- glm(formula = diabetes ~.,
              data = data_train %>% select(-Set, -Fold),
              family = binomial)
```
```{r, echo = FALSE}
lr_mod
```
To visualise the influence of a certain variable on the prediction probability, we took the following procedure.

(1) To investigate the influence of one variable, we fix all other variables at their mean.
(2) Create a sequence of 1000 points between the minimum and maximum observed value of the variable of interest.
(3) Compute the prediction probability for every point: we obtain 1000 prediction probabilities.

These plots are in accordance with the coefficient values we extracted from the model. Namely, we see see a decrease in probability when the values of predictors with negative coefficients increase, and an increase for the predictors that have a positive coefficient.
We also see that predictors whose coefficient is high in absolute value, like pedigree, display a detectable change in probability inside a small range of values. And vice versa, predictors with a low absolute value coefficient, like insulin, need a larger increase in their value for a visible change in probability. This becomes apparent by looking at the x axes. (Note that x axis ranges from 0 to 2.5). 
One surprising thing about these plots is that we see a linear relationship between the probability and many predictors, when we would expect a sigmoid curve, similar to the one for glucose.

```{r, include = FALSE}
means <- data_train %>% 
  select(-diabetes,-Set) %>% 
  apply(MARGIN = 2, FUN = mean)

plot_data <- list()
for(i in 1:8){
  x <- seq(min(data_train[,i]), max(data_train[,i]), length.out = 1000)
  data <- matrix( means[-i] %>% rep(each = 1000), nrow = 1000)
  data <- data.frame(x,data)
  colnames(data) <- c(names(means[i]), names(means[-i]))
  
  y <- predict(lr_mod, newdata = data, type = "response")
  
  plot_data[[i]] <- data.frame(x,y)
}

plot_pregnant <- plot_data[[1]] %>% 
  ggplot(aes(x = x, y = y)) +
  geom_line() +
  labs(x = "pregnant",
       y = "probability") +
  ylim(0,1) +
  theme_minimal()

plot_glucose <- plot_data[[2]] %>% 
  ggplot(aes(x = x, y = y)) +
  geom_line() +
  labs(x = "glucose",
       y = "probability") +
  ylim(0,1) +
  theme_minimal()

plot_pressure <- plot_data[[3]] %>% 
  ggplot(aes(x = x, y = y)) +
  geom_line() +
  labs(x = "pressure",
       y = "probability") +
  ylim(0,1) +
  theme_minimal()

plot_triceps <- plot_data[[4]] %>% 
  ggplot(aes(x = x, y = y)) +
  geom_line() +
  labs(x = "triceps",
       y = "probability") +
  ylim(0,1) +
  theme_minimal()

plot_insulin <- plot_data[[5]] %>% 
  ggplot(aes(x = x, y = y)) +
  geom_line() +
  labs(x = "insulin",
       y = "probability") +
  ylim(0,1) +
  theme_minimal()

plot_mass <- plot_data[[6]] %>% 
  ggplot(aes(x = x, y = y)) +
  geom_line() +
  labs(x = "mass",
       y = "probability") +
  ylim(0,1) +
  theme_minimal()

plot_pedigree <- plot_data[[7]] %>% 
  ggplot(aes(x = x, y = y)) +
  geom_line() +
  labs(x = "pedigree",
       y = "probability") +
  ylim(0,1) +
  theme_minimal()

plot_age <- plot_data[[8]] %>% 
  ggplot(aes(x = x, y = y)) +
  geom_line() +
  labs(x = "age",
       y = "probability") +
  ylim(0,1) +
  theme_minimal()
```

```{r, echo = FALSE}
plot_grid(plot_pregnant, plot_glucose, plot_pressure,
          plot_triceps, plot_insulin, plot_mass,
          plot_pedigree, plot_age)
```

### Performance Results
To be able to compare the performance of LR with other models, we construct the confusion matrix of predictions on the test data. From this confusion matrix, we compute the metrics that we think are most important.
Here, we will list the metrics that we will use, alongside the abbreviations we used in the rest of the document.
TP-> True positive. The number of cases that were correctly classified as having diabetes
TN-> True negative. The number of cases  that were correctly classified as not having diabetes
FN -> False negatives. The number of cases that were incorrectly classified as not having diabetes, when in reality they do.
FP-> False positives. The number of cases that were incorrectly classified as having diabetes while being healthy.

ACC -> Accuracy. The proportion of correctly classified cases.
TPR-> True positive rate/ sensitivity. The proportion of cases with diabetes that were correctly classified as having diabetes.
TNR-> True negative rate/ specificity. The proportion of healthy cases that were correctly classified as being healthy.
PPV->Positive predictive value/precision. The proportion of predicted positive samples that really have diabetes.
NPV->Negative predictive value. The proportion of predicted healthy samples that are truly healthy.
F1-> F1 score. It's the harmonic mean of Positive predictive value and sensitivity. We use it as a measure of overall performance. It has the advantage of being unaffected by class imbalance.

ROC curves will also be used since they are a great way to simultaneously visualize sensitivity and specificity throughout the range of possible thresholds.
The AUC (area under the curve) is a good measure of the overall performance, and will also be used to compare the different models.



```{r, echo = FALSE}
lr_pred_probs <- predict(lr_mod, newdata = data_test, type = "response")
lr_preds <- factor(lr_pred_probs > .5, labels = c("neg", "pos"))

lr_confmat <- table(True = data_test$diabetes, Predicted = lr_preds)
lr_confmat
#calculate the most important metrics
TN <- lr_confmat[1, 1]
FN <- lr_confmat[2, 1]
FP <- lr_confmat[1, 2]
TP <- lr_confmat[2, 2]



tibble(
  ACC = (TP + TN) / sum(lr_confmat),
  TPR = TP / (TP + FN),
  TNR = TN / (TN + FP),
  PPV = TP / (TP + FP),
  NPV = TN / (TN + FN),
  F1 = 2*((PPV*TPR)/(PPV+TPR))
)

#ROC
roc_lr <- roc(data_test$diabetes, lr_pred_probs)
ggroc(roc_lr) + theme_minimal() + labs(title = "Logistic Regression")

roc_lr
```
We see that our model classifies 0.7857143 of the unseen cases correctly. Looking more into the metrics we realise that out of the patients with diabetes only 64% of them were correctly identified by our logistic regression model. That is also shown by the F1 value which is significantly lower that the accuracy. 
Nevertheless, we see an area under the curve of 0.8463, which is impressive for such a simple model.


# Simple model: Classification Tree
### Approach
The second model that we will consider is the classification tree. The idea of such a tree is to recursively make splits based on the variables. Each split should make the observations within one of the resulting groups as similar as possible. The prediction rule of the tree is defined as "majority vote": within a leaf node, determine the class with maximal frequency among the observation within that node. If two or more classes have equal frequency, then choose uniformly random.  
An important notion to make when considering classification trees is the tree complexity. A classification tree has a high complexity when it makes a lot of splits, resulting in a large/deep tree with many leaf nodes that contain few observations. In that case, there is a high variance among the predictions. To prevent this, one can *prune* the tree: stop splitting when some condition is met. For instance, one could

- stop splitting when the number of observations in a node is below some threshold $n_{min}$.
- stop splitting when the complexity of the tree is larger then some threshold $cp$.

Note that there are more ways to prune a tree (or complexity parameters to specify). A list of all complexity parameters and their default values can be found in the documentation of `rpart.control`. Note the default values $n_{min} = 20$ and $cp = 0.01$.  
We will tune the *hyperparameters* $n_{min}$ and $cp$ by means of the 10-fold cross-validation as specified earlier. The metric of performance that we will use is the accuracy. For the possible values of the hyperparameters, we consider $56$ combinations:

- $n_{min}\in\{50\cdot i|i=0,...,6\}$, as $n_{min}=0$ resembles the full grown tree and $n_{min}=300$ gives a tree with just one split (with all other hyperparameters equal to their default value).
- $cp\in\{0.01\cdot i|i=0,...,7\}$, as $cp=0$ resembles the full grown tree and $cp=0.07$ gives a tree with just one split (with all other hyperparameters equal to their default value).

### Training Results
The results of the 10-fold cross-validation can be found below. For the implementation of the cross-validation, please read the .Rmd file. From the table we can read that the tree with $n_{min}=50$ and $cp = 0$ performs best: its mean accuracy over the folds equals approximately 0.76. With the hyperparameters set, we fit the classification tree again on the entire dataset. The resulting tree can be found below the table.


```{r, include = FALSE}
nmins <- seq(0,300, by = 50)
cps <- seq(0,0.07, by = 0.01)

accuracies <- matrix(nrow = length(nmins), ncol = length(cps))
colnames(accuracies) <- paste("cp =", as.character(cps))
rownames(accuracies) <- paste("n_min =", as.character(nmins))

for(i in 1:length(nmins)){
  for(j in 1:length(cps)){
    mean_acc <- 0
    for(k in 1:10){
      train <- data_train %>% filter(Fold != k) %>% select(-Set, -Fold)
      val <- data_train %>% filter(Fold == k)
      ctree <- rpart(diabetes ~.,
                     data = train,
                     control = rpart.control(minsplit = nmins[i], cp = cps[j]))
      preds <- predict(ctree, newdata = val, type = "class")
      acc <- sum(val$diabetes == preds)/nrow(val)
      mean_acc <- mean_acc + acc/10
    }
    accuracies[i,j] <- mean_acc
  }
}
```

```{r, echo = FALSE}
"Mean accuracy over 10 folds. The rows represent n_min, the columns represent cp."
accuracies

hyperparams <- which(accuracies == max(accuracies), arr.ind = TRUE)
ctree_mod <- rpart(diabetes ~.,
                   data = data_train %>% select(-Set, -Fold),
                   control = rpart.control(minsplit = (hyperparams[1]-1)*50, cp = (hyperparams[2]-1)*0.01))
rpart.plot(ctree_mod)
```

It is interesting to see that the classification tree does multiple splits on `glucose`. It seems that this variable contains a lot of helpful information. We see that idea confirmed by the computed variable importance, see the barplot below.  
The second thing that we noted is the following. Although `insulin` is the second most important variable, the tree does not perform a split on that variable. We are unsure why this happens.  
The last interesting notion is that the tree predicts an observation with `pregnant` greater than 7 and `glucose` between 96 and 124 as *diabetes*. We see that the leaf in which this observation ends up contains 8% of the training observations (about 49 observations). Among this 49 observations, there is a slight majority with class *diabetes*.

```{r, echo = FALSE}
data.frame(Variable = names(ctree_mod$variable.importance),
                           Importance = ctree_mod$variable.importance) %>% 
  ggplot(aes(x = Variable, y = Importance, fill = Variable)) +
  geom_bar(stat = "identity", show.legend = FALSE) +
  labs(title = "Variable importance for the classification tree.") +
  theme_minimal()
```

### Performance results
To be able to compare the performance of LR with other models, we construct the confusion matrix of predictions on the test data. From this confusion matrix, we compute the metrics that we think are most important.

```{r, echo = FALSE}
ctree_preds <- predict(ctree_mod, newdata = data_test, type = "class")

ctree_confmat <- table(True = data_test$diabetes, Predicted = ctree_preds)
ctree_confmat
#calculate the most important metrics
TN_tree <- ctree_confmat[1, 1]
FN_tree <- ctree_confmat[2, 1]
FP_tree <- ctree_confmat[1, 2]
TP_tree <- ctree_confmat[2, 2]

tibble(
  ACC = (TP_tree + TN_tree) / sum(ctree_confmat),
  TPR = TP_tree / (TP_tree + FN_tree),
  TNR = TN_tree / (TN_tree + FP_tree),
  PPV = TP_tree / (TP_tree + FP_tree),
  NPV = TN_tree / (TN_tree + FN_tree),
  F1 = 2*((PPV*TPR)/(PPV+TPR))
)
#ROC
ctree_preds_prob <- predict(ctree_mod, newdata = data_test)
class(ctree_preds_prob)
roc_ctree <- roc(data_test$diabetes, ctree_preds_prob[,2])
ggroc(roc_ctree) + theme_minimal() + labs(title = "Classification tree")
roc_ctree
```

We see that our model classifies 0.7662338 of the unseen cases correctly. We can again understand from looking at the rest of the metrics that the high accuracy is mostly because of the correct classification of healthy patients, which make up the majority of the cases.
We see an area under the curve of 0.7554, which is significantly lower than the logistic regression model.

# Complex Model: Random Forest 
### Approach
The final model under investigation is the Random Forest model, which expands on the decision tree model by building several classification trees and leveraging "The wisdom of the crowd". The random forest model further adds to the complexity of a single decision tree by de-correlating the predictor variables. This is done by sampling a random number of features in the assessment of each split, for each tree of the ensemble. The Random forest model was implemented by calling the 'train' function in the 'caret' package. It should be noted that any parameters not mentioned were set to their default value. Hyper-parameter search was not conducted in an exhaustive fashion to extract the global minimum. Instead, values around the default for each mentioned hyper-parameter were chosen where possible, and 10-fold cross validation was used in order to find the hyper-parameter values resulting in the highest mean accuracy measure against the training-set. View .Rmd file for more information on the implementation. 

Several hyper-parameters were optimized in the creation of the model: 

- $mtry$ : The number of predictor variables randomly sampled without replacement at the assessment of each split. It should be noted that since sampling without replacement
occurs at the node level, it is still possible to split on the same attribute multiple times in a tree. mtry was the first optimized hyper-parameter, by convention: $mtry = \sqrt(Number of predictors)$. Normally, one would take values around this default in the tuning process. However,since the number of predictor variables is only 8, it is possible to assess splitting on 1-8 variables. The resulting value of $mtry =6$ resulted in the highest mean accuracy of ~ 76.72%. This value of $mtry$ was utilized for tuning the remaining hyper-parameters. 

- $maxnodes$: An important hyper-parameter to fend-off against over-fitting. This parameter places a constraint on the maximum number of leaf-nodes grown, for each tree in the ensemble. $maxnodes$ was optimized by taking a range of values from 5 to 15. The resulting mean accuracy scores did not confidently seem to taper to a lower value. Therefore, the range was extended to accommodate up to 30 $maxnodes$. The resultant $maxnodes$ value with the highest mean accuracy (~ 75.74 %) was $maxnodes = 13$   

- $ntree$: The number of trees grown in the ensemble, for which the results will be averaged to return the final classification of the random forest.The default value for $ntree$ passed in the 'train' function is 500. The values explored in determining the best value for $ntree$ ranged from 400-1000 trees grown with an incremental increase of 50 trees. $ntree = 550$ resulted in the highest mean accuracy score of 76.07%. 

- $nodesize$: Refers to the minimum number of observations required to be in the child-node if the parent node were to split. A higher $nodesize$ results in smaller trees, while a low value for $nodesize$ can risk over-fitting to the training data if unrestrained by other means. The default value for $noidesize$ in a random forest classification task is 1. Values were chosen in the range of 1:20.The highest mean accuracy in this cohort was 77.21%, resulting in $nodesize = 12$.

### Training results
```{R , Insert = FALSE}
# Define the control
trControl <- caret::trainControl(method = "cv",
    number = 10,
    search = "grid")

#Investigate values for mtry between 1 and 8
tuneGrid <- expand.grid(.mtry = 1: 8)
rf_mtry <- train(diabetes ~ .,
    data = data_train %>% select(-Set, -Fold),
    method = "rf",
    metric = "Accuracy",
    tuneGrid = tuneGrid,
    trControl = trControl,
    importance = TRUE)

#store best mtry
best_mtry <- rf_mtry$bestTune$mtry 
best_mtry

#Try different values for maxnodes
store_maxnode <- list()
tuneGrid <- expand.grid(.mtry = best_mtry)
for (maxnodes in 5: 15) { 
      set.seed(1234)
    rf_maxnode <- train(diabetes ~ .,
    data = data_train %>% select(-Set, -Fold),
        method = "rf",
        metric = "Accuracy",
        tuneGrid = tuneGrid,
        trControl = trControl,
        importance = TRUE,
        maxnodes = maxnodes)
    current_iteration <- toString(maxnodes)
    store_maxnode[[current_iteration]] <- rf_maxnode
}
results_mtry <- resamples(store_maxnode)
summary(results_mtry)

#Try a longer range.
store_maxnode <- list()
tuneGrid <- expand.grid(.mtry = best_mtry)
for (maxnodes in c(15: 30)) {
      set.seed(1234)
    rf_maxnode <- train(diabetes ~ .,
    data = data_train %>% select(-Set, -Fold),
        method = "rf",
        metric = "Accuracy",
        tuneGrid = tuneGrid,
        trControl = trControl,
        importance = TRUE,
        maxnodes = maxnodes)
    key <- toString(maxnodes)
    store_maxnode[[key]] <- rf_maxnode
}
results_node <- resamples(store_maxnode)
summary(results_node)

#search for best ntree 
store_maxtrees <- list()
ntrees <- c(450, 500, 550, 600, 650, 700, 750, 800, 850, 900, 950, 1000)
for (ntree in ntrees) {
      set.seed(5678)
      rf_maxtrees <- train(diabetes ~ .,
      data = data_train %>% select(-Set, -Fold),
      method = "rf",
      metric = "Accuracy",
      tuneGrid = tuneGrid,
      trControl = trControl,
      importance = TRUE,
      maxnodes = 18,
      ntree = ntree)
  key <- toString(ntree)
  store_maxtrees[[key]] <- rf_maxtrees
}
results_tree <- resamples(store_maxtrees)
summary(results_tree)

#search for best nodesize 
store_nodesize <- list()
tuneGrid <- expand.grid(.mtry = best_mtry)
for (nodesize in 1: 20) {
      set.seed(5678)
      rf_nodesize <- train(diabetes ~ .,
      data = data_train %>% select(-Set, -Fold),
      method = "rf",
      metric = "Accuracy",
      tuneGrid = tuneGrid,
      trControl = trControl,
      importance = TRUE,
      nodesize = nodesize)
    current_iteration <- toString(nodesize)
    store_nodesize[[current_iteration]] <- rf_nodesize
}
results_mtry <- resamples(store_nodesize)
summary(results_mtry)
```

The hyper-parameter optimization for the Random Forest model resulted in the tuned values $mtry = 6$, $maxnodes=13$, $ntree=550$ and $nodesize=12$. Training resulted in the following variable importance plot. 

```{r, Insert = FALSE}
#Train the forest
fit_rf <- train(diabetes ~ .,
    data = data_train %>% select(-Set, -Fold),
    method = "rf",
    metric = "Accuracy",
    tuneGrid = tuneGrid,
    trControl = trControl,
    importance = TRUE,
    ntree = 550,
    nodesize = 12,
    maxnodes = 13)

```

The above training protocol resulted of the Random Forest Model resulted in the following confusion matrix when predicting on the test-set, and the following variable importance plot. 

```{r}
#Evaluate
rf_preds <- predict(fit_rf, newdata= data_test)
rf_matrix <- table(True = data_test$diabetes, Predicted = rf_preds)
rf_matrix

```
```{r, echo = FALSE}
rf_impvar <- varImp(fit_rf)
plot(rf_impvar, main = "Random Forest Variable importance")
```

```{r, echo = FALSE}
#Evaluate
rf_confmat <- table(True = data_test$diabetes, Predicted = rf_preds)
#calculate the most important metrics
TN_rf <- rf_confmat[1, 1]
FN_rf <- rf_confmat[2, 1]
FP_rf <- rf_confmat[1, 2]
TP_rf <- rf_confmat[2, 2]

tibble(
  ACC = (TP_rf + TN_rf) / sum(rf_confmat),
  TPR = TP_rf / (TP_rf + FN_rf),
  TNR = TN_rf / (TN_rf + FP_rf),
  PPV = TP_rf / (TP_rf + FP_rf),
  NPV = TN_rf / (TN_rf + FN_rf),
  F1 = 2*((PPV*TPR)/(PPV+TPR))
)
#ROC
rf_preds_prob <- predict(fit_rf, newdata = data_test, type = "prob" )
roc_rf <- roc(data_test$diabetes, rf_preds_prob[,2])
ggroc(roc_rf) + theme_minimal() + labs(title = "Random Forest")

roc_rf
```
Finally, our random forest has an accuracy of 0.7792208, and 66% of the diseased patients were crrectly classified, which is the highest we have seen. 
The area under the curve for our random forest is also the highest of all of our models. 
We see that our random forest significantly outperforms its simplified equivalent, and slightly outperforms the logistic regression.
###McNemar Test results. 
Put Mcnemar results here
```{r}
#Put Mcnemar results here, test for: 
#classification vs Logistic. 
#classification vs random forest
#random forest vs classifcation 
```

